{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test searching, reranker and answer generation\n",
    "\n",
    "This code demonstrate how to test the search with different options (uppercase/lowercase, combination of vector fields, number of documents retrieved in the search and number of documents used in the answer generation), re-rank the retrieve documents and generate and evaluate the answers creating excel files with the different combinations.\n",
    "\n",
    "The tests are defined in the constact TESTS with the following fields:\n",
    "+ test-name: it will be used as the Excel file name\n",
    "+ embeddings_fields: list of vector fields to be used in the search\n",
    "+ uppercase/lowercase: the query will be converted to uppercase or lowercase to execute the search\n",
    "+ embbeding_model: ada or large-3\n",
    "+ index_name: the name of the index created with the notebook 'create_index_and_index_documents.ipynb'\n",
    "+ max_retrieve: maximum number of search results\n",
    "+ max_generate: maximum number of documents (chunks) used to generate the answers\n",
    "\n",
    "The output is the Excels files with the tests results.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "+ An Azure subscription, with [access to Azure OpenAI](https://aka.ms/oai/access).\n",
    "+ An Azure OpenAI service with the service name and an API key.\n",
    "+ A deployment of the text-embedding-ada-002 embedding model on the Azure OpenAI Service with the deployment name 'ada'.\n",
    "+ A deployment of the text-embedding-3-large embedding model on the Azure OpenAI Service with the deployment name 'ada'.\n",
    "+ An Azure AI Search service with the end-point, API Key and the index name to create.\n",
    "\n",
    "We used Python 3.12.3, [Visual Studio Code with the Python extension](https://code.visualstudio.com/docs/python/python-tutorial), and the [Jupyter extension](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter) to test this example.\n",
    "\n",
    "### Set up a Python virtual environment in Visual Studio Code\n",
    "\n",
    "1. Open the Command Palette (Ctrl+Shift+P).\n",
    "1. Search for **Python: Create Environment**.\n",
    "1. Select **Venv**.\n",
    "1. Select a Python interpreter. Choose 3.10 or later.\n",
    "\n",
    "It can take a minute to set up. If you run into problems, see [Python environments in VS Code](https://code.visualstudio.com/docs/python/environments)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install openai\n",
    "! pip install azure-search-documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages and create AOAI clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from pa_utils import call_aoai, semantic_hybrid_search_with_filter, get_filtered_chunks, generate_answer\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# AZURE AI SEARCH\n",
    "ai_search_endpoint = os.environ[\"SEARCH_SERVICE_ENDPOINT\"]\n",
    "ai_search_apikey = os.environ[\"SEARCH_SERVICE_QUERY_KEY\"]\n",
    "ai_search_index_name = os.environ[\"SEARCH_INDEX_NAME\"]\n",
    "ai_search_credential = AzureKeyCredential(ai_search_apikey)\n",
    "\n",
    "aoai_api_version = '2024-02-15-preview'\n",
    "\n",
    "# AOAI FOR ANSWER GENERATION\n",
    "aoai_answer_endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "aoai_answer_apikey = os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "aoai_answer_model_name = os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"]\n",
    "# Create AOAI client for answer generation\n",
    "aoai_answer_client = AzureOpenAI(\n",
    "    azure_deployment=aoai_answer_model_name,\n",
    "    api_version=aoai_api_version,\n",
    "    azure_endpoint=aoai_answer_endpoint,\n",
    "    api_key=aoai_answer_apikey\n",
    ")\n",
    "\n",
    "# AZURE OPENAI FOR RERANKING\n",
    "aoai_rerank_endpoint = os.environ[\"AZURE_OPENAI_RERANK_ENDPOINT\"]\n",
    "azure_openai_rerank_key = os.environ[\"AZURE_OPENAI_RERANK_API_KEY\"]\n",
    "rerank_model_name = os.environ[\"AZURE_OPENAI_RERANK_DEPLOYMENT_NAME\"]\n",
    "# Create AOAI client for reranking\n",
    "aoai_rerank_client = AzureOpenAI(\n",
    "    azure_deployment=rerank_model_name,\n",
    "    api_version=aoai_api_version,\n",
    "    azure_endpoint=aoai_rerank_endpoint,\n",
    "    api_key=azure_openai_rerank_key\n",
    ")\n",
    "\n",
    "# AZURE OPENAI FOR EMBEDDING\n",
    "aoai_embedding_endpoint = os.environ[\"AZURE_OPENAI_EMBEDDING_ENDPOINT\"]\n",
    "azure_openai_embedding_key = os.environ[\"AZURE_OPENAI_EMBEDDING_API_KEY\"]\n",
    "embedding_model_name_ada = os.environ[\"AZURE_OPENAI_EMBEDDING_NAME_ADA\"]\n",
    "embedding_model_name_large_3 = os.environ[\"AZURE_OPENAI_EMBEDDING_NAME_LARGE_3\"]\n",
    "# Create AOAI client for embedding creation (ADA)\n",
    "aoai_embedding_client_ada = AzureOpenAI(\n",
    "    azure_deployment=embedding_model_name_ada,\n",
    "    api_version=aoai_api_version,\n",
    "    azure_endpoint=aoai_embedding_endpoint,\n",
    "    api_key=azure_openai_embedding_key\n",
    ")\n",
    "# Create AOAI client for embedding creation (Large-3)\n",
    "aoai_embedding_client_large_3 = AzureOpenAI(\n",
    "    azure_deployment=embedding_model_name_large_3,\n",
    "    api_version=aoai_api_version,\n",
    "    azure_endpoint=aoai_embedding_endpoint,\n",
    "    api_key=azure_openai_embedding_key\n",
    ")\n",
    "\n",
    "# CONSTANTS\n",
    "SELECT_FIELDS=[\"id\", \"title\", \"content\"] # Fields to retrieve in the search\n",
    "QUERY_LANGUAGE=\"en-US\" # Query language\n",
    "\n",
    "# Test-name: embeddings_fields | uppercase/lowercase | embbeding_model | index_name | max_retrieve | max_generate\n",
    "TESTS = {\n",
    "        \"title_content_large3_512_search_upper_20_10\": (\"embeddingTitle, embeddingContent\", \"upper\", \"large-3\", \"project_assurance_large_3\", 20, 10),\n",
    "        \"title_content_large3_512_search_upper_20_20\": (\"embeddingTitle, embeddingContent\", \"upper\", \"large-3\", \"project_assurance_large_3\", 20, 20),\n",
    "        \"title_content_large3_512_search_lower_20_10\": (\"embeddingTitle, embeddingContent\", \"lower\", \"large-3\", \"project_assurance_large_3\", 20, 10),\n",
    "        \"title_content_large3_512_search_lower_20_20\": (\"embeddingTitle, embeddingContent\", \"lower\", \"large-3\", \"project_assurance_large_3\", 20, 20),\n",
    "}\n",
    "#        \"title_content_ada_512_search_upper_20_10\": (\"embeddingTitle, embeddingContent\", \"upper\", \"ada\", \"find_duplicates_4\", 20, 10),\n",
    "#        \"title_content_ada_512_search_upper_20_20\": (\"embeddingTitle, embeddingContent\", \"upper\", \"ada\", \"find_duplicates_4\", 20, 20),\n",
    "#        \"title_content_ada_512_search_lower_20_10\": (\"embeddingTitle, embeddingContent\", \"lower\", \"ada\", \"find_duplicates_4\", 20, 10),\n",
    "#        \"title_content_ada_512_search_lower_20_20\": (\"embeddingTitle, embeddingContent\", \"lower\", \"ada\", \"find_duplicates_4\", 20, 20),\n",
    "\n",
    "QA_WITH_ANSWERS_FILENAME = 'qa_pairs.xlsx' #'QA_with_answers.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_answer(question, correct_answer, answer):\n",
    "    \n",
    "    system_prompt = \"\"\"You are an AI assistant that helps people validate the accuracy and completeness of a response against a ground trust. Given the user's question, the expected ground truth answer and the current answer generated by a RAG pattern, compare the meaning of both answers and assess if the current answer addresses the user's question and select a number that best describes this assessment considering the following guidelines:\n",
    "    - 0: The generated answer and the expected answer have completely different meanings, and the generated answer does not address the user's question.\n",
    "    - 1: The generated answer is very similar in meaning to the expected answer but lacks some crucial information, and it partially addresses the user's question.\n",
    "    - 2: The generated answer is well-aligned with the expected answer, capturing the main points accurately, and fully addressing the user's question.\n",
    "    - 3: The generated answer not only aligns with the expected ground truth and answers the user's question but also adds valuable additional details or insights.\n",
    "    Based on these guidelines, provide only the number that best represents the relationship between the generated answer and the expected ground truth answer. Do not include any explaination, only the number.\n",
    "    \"\"\"\n",
    "    \n",
    "    user_prompt = f'\\nQuestion: {question}\\nExpected Ground Truth Answer: \"{correct_answer}\\nGenerated Answer: \"{answer}\"\\n\"\\nYour evaluation: '\n",
    "\n",
    "    return call_aoai(aoai_answer_client, aoai_answer_model_name, system_prompt, user_prompt, 0.0, 800)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answer_iddoc(text, ids_doc, all=False):\n",
    "    #print(f'ANSWER TO CHECK: [{text}]')\n",
    "\n",
    "    ids = ids_doc.split(', ')\n",
    "    if all: # All the IDs must appears to be 1\n",
    "        for id in ids:\n",
    "            if id in text:\n",
    "                continue\n",
    "            else:\n",
    "                return 0\n",
    "        return 1\n",
    "    \n",
    "    else: # If any ID is included is 1\n",
    "        for id in ids:\n",
    "            if id in text:\n",
    "                return 1\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def execute_test(test_name, embedding_fields, case, embedding_model, index_name, max_retrieve, max_generate, q_a_filename_in=QA_WITH_ANSWERS_FILENAME):\n",
    "\n",
    "    dir_out = \"data_out\"\n",
    "    os.makedirs(dir_out,exist_ok=True)\n",
    "    data_in = pd.read_excel(q_a_filename_in)\n",
    "\n",
    "    print(f'test_name: {test_name}')\n",
    "    print(f\"\\t embeddings_fields: {embedding_fields}\")\n",
    "    print(f\"\\t case: {case}\")\n",
    "    print(f\"\\t embedding_model: {embedding_model}\")\n",
    "    print(f\"\\t index_name: {index_name}\")\n",
    "    print(f\"\\t max_retrieve: {max_retrieve}\")\n",
    "    print(f\"\\t max_generate: {max_generate}\")\n",
    "\n",
    "    #data_out = [\n",
    "    #    ['QUESTION', 'EXPECTED_ANSWER', 'EXPECTED_ID_DOCS', 'ANSWER_WITH_ANSWERS', 'EVALUATION_GPT', 'ID_IN_ANSWER', 'ID_IN_SEARCH', 'ANSWER_WITH_CHUNKS', 'EVALUATION_GPT', 'ID_IN_ANSWER', 'ID_IN_SEARCH']\n",
    "    #]\n",
    "\n",
    "    data_out = {'QUESTION': [],\n",
    "            'EXPECTED_ANSWER': [],\n",
    "            #'EXPECTED_ID_DOCS': [],\n",
    "            'ANSWER_WITH_ANSWERS': [],\n",
    "            'EVALUATION_GPT_AA': [],\n",
    "            #'ID_IN_ANSWER_AA': [],\n",
    "            #'ID_IN_SEARCH_AA': [],\n",
    "            'ANSWER_WITH_CHUNKS': [],\n",
    "            'EVALUATION_GPT_AC': [],\n",
    "            #'ID_IN_ANSWER_AC': [],\n",
    "            #'ID_IN_SEARCH_AC': []\n",
    "    }\n",
    "\n",
    "    # Create Azure AI Search client\n",
    "    ai_search_client = SearchClient(endpoint=ai_search_endpoint, index_name=index_name, credential=ai_search_credential)\n",
    "\n",
    "    # FOR EVERY Q&A FILE IN THE INPUT FILE\n",
    "    for index, row in data_in.iterrows():\n",
    "        print(f\"Row {index + 1}: =====================================================\")\n",
    "\n",
    "        user_question = row[\"question\"] # Valor de la columna Pregunta\n",
    "        if case == 'upper':\n",
    "            user_question = user_question.upper()\n",
    "        else:\n",
    "            user_question = user_question.lower()\n",
    "\n",
    "        respuesta_best = row[\"answer\"] # Valor de la columna de Respuesta Esperada\n",
    "        #docs_best = str(row[\"DOCS_BEST\"]) # Valor de la columna de Respuesta Esperada\n",
    "\n",
    "        data_out['QUESTION'].append(user_question)          # EXCEL COLUMN: QUESTION\n",
    "        data_out['EXPECTED_ANSWER'].append(respuesta_best)  # EXCEL COLUMN: EXPECTED_ANSWER\n",
    "        #data_out['EXPECTED_ID_DOCS'].append(docs_best)       # EXCEL COLUMN: EXPECTED_ID_DOCS\n",
    "\n",
    "        # SEMANTIC HYBRID SEARCH\n",
    "        if embedding_model == 'ada':\n",
    "            embedding_client = aoai_embedding_client_ada\n",
    "        else:\n",
    "            embedding_client = aoai_embedding_client_large_3\n",
    "\n",
    "        results = semantic_hybrid_search_with_filter(ai_search_client, user_question.lower(), embedding_client, embedding_model, embedding_fields, max_retrieve, SELECT_FIELDS, QUERY_LANGUAGE) # SEARCH\n",
    "        \n",
    "        # Re-rank the chunks\n",
    "        data = get_filtered_chunks(aoai_rerank_client, rerank_model_name, results, user_question, max_retrieve)\n",
    "\n",
    "        if max_retrieve > max_generate: # If the max number of docs to retrieve is higher than max number of docs to use in answer generation, sort them by confidence and leave only the max number of docs to generate\n",
    "            if data != None:\n",
    "                #sorted_data = sorted(data, key=lambda x: x[\"confidence\"], reverse=True)\n",
    "                sorted_data = sorted(data, key=lambda x: x.get('confidence', float('-inf')), reverse=True)\n",
    "                data=sorted_data[:max_generate]\n",
    "\n",
    "        ids_in_search_results = \"\"\n",
    "        for result in results:\n",
    "            ids_in_search_results = ids_in_search_results + ' ' + result['id']\n",
    "\n",
    "        # Si quedan contenidos para generar la respuesta\n",
    "        if data != []:\n",
    "            # ANSWER_WITH_ANSWERS: Generate the answer with the answers generated by the re-ranker with filtered chunks\n",
    "            answer = generate_answer(aoai_answer_client, aoai_answer_model_name, data, user_question, 'answer')\n",
    "            print(f'RESPONSE WITH ANSWERS: [{answer}]')\n",
    "            data_out['ANSWER_WITH_ANSWERS'].append(answer)                                                # EXCEL COLUMN: ANSWER_WITH_ANSWERS\n",
    "            data_out['EVALUATION_GPT_AA'].append(evaluate_answer(user_question, respuesta_best, answer))  # EXCEL COLUMN: EVALUATION_GPT_AA\n",
    "            #data_out['ID_IN_ANSWER_AA'].append(check_answer_iddoc(answer, docs_best))                    # EXCEL COLUMN: ID_IN_ANSWER_AA\n",
    "            #data_out['ID_IN_ANSWER_AA'].append(check_answer_iddoc(ids_in_search_results, docs_best))     # EXCEL COLUMN: ID_IN_SEARCH_AA\n",
    "            print('------------------------------------------------------------------')\n",
    "        \n",
    "            # ANSWER_WITH_CHUNKS - Generate the answer with the chunks filtered by the re-ranker\n",
    "            answer = generate_answer(aoai_answer_client, aoai_answer_model_name, data, user_question, 'content')\n",
    "            print(f'RESPONSE WITH CHUNKS: [{answer}]')\n",
    "            data_out['ANSWER_WITH_CHUNKS'].append(answer)                                                 # EXCEL COLUMN: ANSWER_WITH_CHUNKS\n",
    "            data_out['EVALUATION_GPT_AC'].append(evaluate_answer(user_question, respuesta_best, answer))  # EXCEL COLUMN: EVALUATION_GPT_AC\n",
    "            #data_out['ID_IN_ANSWER_AC'].append(check_answer_iddoc(answer, docs_best))                    # EXCEL COLUMN: ID_IN_ANSWER_AC\n",
    "            #data_out['ID_IN_SEARCH_AC'].append(check_answer_iddoc(ids_in_search_results, docs_best))     # EXCEL COLUMN: ID_IN_SEARCH_AC\n",
    "\n",
    "            print('------------------------------------------------------------------')\n",
    "\n",
    "        else:\n",
    "            answer = 'There is not any content to generate the answer'\n",
    "            data_out['ANSWER_WITH_ANSWERS'].append(answer)  # EXCEL COLUMN: ANSWER_WITH_ANSWERS\n",
    "            data_out['EVALUATION_GPT_AA'].append(0)         # EXCEL COLUMN: EVALUATION_GPT_AA\n",
    "            #data_out['ID_IN_ANSWER_AA'].append(0)           # EXCEL COLUMN: ID_IN_ANSWER_AA\n",
    "            #data_out['ID_IN_SEARCH_AA'].append(0)           # EXCEL COLUMN: ID_IN_SEARCH_AA\n",
    "            data_out['ANSWER_WITH_CHUNKS'].append(answer)   # EXCEL COLUMN: ANSWER_WITH_CHUNKS\n",
    "            data_out['EVALUATION_GPT_AC'].append(0)         # EXCEL COLUMN: EVALUATION_GPT_AC\n",
    "            #data_out['ID_IN_ANSWER_AC'].append(0)           # EXCEL COLUMN: ID_IN_ANSWER_AC\n",
    "            #data_out['ID_IN_SEARCH_AC'].append(0)           # EXCEL COLUMN: ID_IN_SEARCH_AC\n",
    "\n",
    "        if index > 5: break # parar después de 4 filas\n",
    "\n",
    "    # Excel file with the results\n",
    "    df = pd.DataFrame(data_out)\n",
    "    filename_out = dir_out + '/' + test_name + '.xlsx'\n",
    "    print(f'Writting file {filename_out}')\n",
    "    df.to_excel(filename_out, index=False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_name, (embedding_fields, case, embedding_model, index_name, max_retrieve, max_generate) in TESTS.items():\n",
    "    execute_test(test_name, embedding_fields, case, embedding_model, index_name, max_retrieve, max_generate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
